# -*- coding: utf-8 -*-
"""4_Images_Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19An9I1FJpeRJpXLCnIFgfVDTvOgpcFwZ
"""

!pip install tensorflow

import os
import math
import numpy as np
import matplotlib.pyplot as plt

import keras
from keras import ops
from keras.layers import Resizing
import tensorflow as tf
import tensorflow_datasets as tfds

# Set backend
os.environ["KERAS_BACKEND"] = "jax"

# Constants
BATCH_SIZE = 32
NUM_CLASSES = 101
IMAGE_SIZE = (224, 224)

# Function to plot images
def plot_image_gallery(images, titles=None, num_cols=3, figsize=(6, 12)):
    num_images = len(images)
    images = np.asarray(images) / 255.0
    images = np.minimum(np.maximum(images, 0.0), 1.0)
    num_rows = (num_images + num_cols - 1) // num_cols
    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize, squeeze=False)
    axes = axes.flatten()
    for i, ax in enumerate(axes):
        if i < num_images:
            ax.imshow(images[i])
            ax.axis("off")
            if titles and len(titles) > i:
                ax.set_title(titles[i], fontsize=12)
        else:
            ax.axis("off")
    plt.show()
    plt.close()

# Load and preprocess dataset
def package_inputs(image, label):
    return {"images": image, "labels": tf.one_hot(label, NUM_CLASSES)}

train_ds, eval_ds = tfds.load(
    "caltech101", split=["train", "test"], as_supervised=True
)
train_ds = train_ds.map(package_inputs, num_parallel_calls=tf.data.AUTOTUNE)
eval_ds = eval_ds.map(package_inputs, num_parallel_calls=tf.data.AUTOTUNE)

# Resize for inference
inference_resizing = Resizing(*IMAGE_SIZE, crop_to_aspect_ratio=True)

def do_resize(inputs):
    inputs["images"] = inference_resizing(inputs["images"])
    return inputs

eval_ds = eval_ds.map(do_resize, num_parallel_calls=tf.data.AUTOTUNE)

# Visualize resized images
image_batch = next(iter(eval_ds.batch(BATCH_SIZE).take(1)))["images"]
plot_image_gallery(image_batch)

# Unpackage dictionary into tuples
def unpackage_dict(inputs):
    return inputs["images"], inputs["labels"]

train_ds = train_ds.map(unpackage_dict, num_parallel_calls=tf.data.AUTOTUNE)
eval_ds = eval_ds.map(unpackage_dict, num_parallel_calls=tf.data.AUTOTUNE)

# Learning rate scheduler
def lr_warmup_cosine_decay(
    global_step,
    warmup_steps,
    hold=0,
    total_steps=0,
    start_lr=0.0,
    target_lr=1e-2,
):
    learning_rate = (
        0.5
        * target_lr
        * (
            1
            + ops.cos(
                math.pi
                * ops.convert_to_tensor(global_step - warmup_steps - hold, dtype="float32")
                / ops.convert_to_tensor(total_steps - warmup_steps - hold, dtype="float32")
            )
        )
    )

    warmup_lr = target_lr * (global_step / warmup_steps)

    if hold > 0:
        learning_rate = ops.where(
            global_step > warmup_steps + hold, learning_rate, target_lr
        )
    else:
        learning_rate = ops.where(
            global_step > warmup_steps, learning_rate, warmup_lr
        )

    learning_rate = ops.where(
        global_step > total_steps, 0.0, learning_rate
    )
    return learning_rate