# -*- coding: utf-8 -*-
"""Text_Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11hGekK3lSNbjZvGZAYJtnd44slKI2ZTA

Easy
"""

import tensorflow_hub as hub
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# Load the embedding layer from TF Hub
embedding = hub.load("https://tfhub.dev/google/Wiki-words-250/1")
embed_fn = embedding.signatures["default"]

# Sample input to get embedding dimension
sample_embedding = embed_fn(tf.constant(["test"]))["default"]
embedding_dim = sample_embedding.shape[-1]
print("Embedding dimension:", embedding_dim)

# Prepare your sample text dataset
texts = ["Example sentence 1", "Example sentence 2", "Example sentence 3"]
labels = [0, 1, 0]

# Tokenize
tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
X = pad_sequences(sequences, padding='post')
word_index = tokenizer.word_index

# Create embedding matrix
embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))

for word, i in word_index.items():
    result = embed_fn(tf.constant([word]))["default"]
    embedding_vector = result.numpy()[0]
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2)

# âœ… Convert to NumPy arrays
y_train = np.array(y_train)
y_test = np.array(y_test)

# Build the model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=len(word_index) + 1,
                              output_dim=embedding_dim,
                              weights=[embedding_matrix],
                              trainable=False),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=5)

"""Intermediate"""

import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.model_selection import train_test_split
import numpy as np

# Load tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Sample data
texts = ["This is a good product.", "I am not happy with this.", "Great service."]
labels = [1, 0, 1]

# Tokenize inputs
inputs = tokenizer(texts, return_tensors='tf', padding=True, truncation=True, max_length=512)

# Convert to numpy
input_ids = inputs['input_ids'].numpy()
attention_mask = inputs['attention_mask'].numpy()
labels = np.array(labels)

# Split
X_train_ids, X_test_ids, X_train_mask, X_test_mask, y_train, y_test = train_test_split(
    input_ids, attention_mask, labels, test_size=0.2, random_state=42
)

# Convert back to tensors
train_inputs = {
    'input_ids': tf.convert_to_tensor(X_train_ids),
    'attention_mask': tf.convert_to_tensor(X_train_mask)
}
test_inputs = {
    'input_ids': tf.convert_to_tensor(X_test_ids),
    'attention_mask': tf.convert_to_tensor(X_test_mask)
}

# Use the optimizer from transformers
from transformers import AdamWeightDecay
optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)

# Compile and train
model.compile(optimizer=optimizer,
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.fit(train_inputs, y_train, epochs=3, batch_size=2)

"""Advanced"""

from transformers import GPT2Tokenizer, TFGPT2LMHeadModel
import tensorflow as tf

# Load GPT-2 pre-trained model and tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = TFGPT2LMHeadModel.from_pretrained("gpt2")

# Encode input text and generate output
input_text = "Once upon a time"
inputs = tokenizer(input_text, return_tensors="tf")

# Generate text
output = model.generate(inputs['input_ids'], max_length=50, num_return_sequences=1)
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)

"""Expert"""

from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load pre-trained T5 model and tokenizer
tokenizer = T5Tokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small")

# Summarize the text
input_text = "The quick brown fox jumps over the lazy dog. The dog is very lazy and does not move much."

# Tokenize and encode input text
inputs = tokenizer("summarize: " + input_text, return_tensors="pt", max_length=512, truncation=True)

# Generate summary
summary_ids = model.generate(inputs['input_ids'], max_length=50, min_length=10, length_penalty=2.0)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print(f"Summary: {summary}")